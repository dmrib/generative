{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "DATA_FOLDER = '../data/CelebA/'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ENCODED_DIMS = 200\n",
    "IMAGE_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=DATA_FOLDER,\n",
    "                               transform = transforms.Compose([\n",
    "                                   transforms.Resize(IMAGE_SIZE),\n",
    "                                   transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "                               ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entire dataset size\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# get train and test sets sizes\n",
    "train_size = int(dataset_size * 0.99)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# split train and test sets\n",
    "train_set, test_set = utils.data.random_split(dataset, (train_size, test_size))\n",
    "\n",
    "# report\n",
    "print(f'CelebA size: {dataset_size} images\\n'\n",
    "      f'Training set size: {train_size} images\\n'\n",
    "      f'Test set size: {test_size} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = utils.data.DataLoader(test_set, batch_size = BATCH_SIZE * 2, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample batch\n",
    "sample = next(iter(train_loader))\n",
    "\n",
    "# create image grid\n",
    "grid = torchvision.utils.make_grid(sample[0].to('cpu')[:64], padding = 2, normalize = True)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.axis('off')\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampler layer definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, mu, log_var):\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        return mu + torch.exp(log_var / 2) * epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder model defition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(4096, ENCODED_DIMS)\n",
    "        self.log_var = nn.Linear(4096, ENCODED_DIMS)\n",
    "        self.sample = Sampler()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        sampled = self.sample(mu, log_var)\n",
    "        \n",
    "        return [mu, log_var, sampled]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(encoder.to(DEVICE), (3, 128, 128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
